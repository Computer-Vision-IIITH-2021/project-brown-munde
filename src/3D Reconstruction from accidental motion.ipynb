{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import unary_from_labels, create_pairwise_bilateral, create_pairwise_gaussian,unary_from_softmax\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_FOLDER_PATH = \"../datasets/custom_still\"\n",
    "\n",
    "\n",
    "# Point Clouds \n",
    "# INITIAL_POINT_CLOUD = '../output/initial_point_cloud.ply'\n",
    "# FINAL_POINT_CLOUD = '../output/final_point_cloud.ply'\n",
    "\n",
    "# Bundle File\n",
    "# BUNDLE_FILE = '../output/bundle.out'\n",
    "\n",
    "# # Shi-Tomasi parameters\n",
    "# feature_params = dict(maxCorners = 5000, \n",
    "#                       qualityLevel = 0.03, \n",
    "#                       minDistance = 10, \n",
    "#                       blockSize = 15\n",
    "#                       )\n",
    "\n",
    "# # Lucas-Kanade parameters\n",
    "# lk_params = dict(   winSize  = (25,25),\n",
    "#                     maxLevel = 8,\n",
    "#                     criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.3))\n",
    "# Ceres-Solver parameters\n",
    "CERES_PARAMS = dict(\n",
    "                    solver = '../ceres-bin/bin/bundle_adjuster',\n",
    "                    maxIterations = 1000,\n",
    "                    input_ply = '../output/initial.ply',\n",
    "                    output_ply = '../output/final.ply',\n",
    "                    inner_iterations = 'true',\n",
    "                    nonmonotonic_steps = 'false'\n",
    "                    )\n",
    "\n",
    "CAMERA_PARAMS = dict(fx=1781,\n",
    "                     fy=1781,\n",
    "                     cx=960,\n",
    "                     cy=540,\n",
    "                     k1=0,\n",
    "                     k2=0,\n",
    "                     s=0,\n",
    "                    )\n",
    "EXTRINSIC_FILE = '../output/extrinsics.csv'\n",
    "\n",
    "\n",
    "\n",
    "# Optical Flow Plot\n",
    "# OPTICAL_FLOW_PLOT = '../output/optical_flow.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Plane Sweep Calculation for photoconsistency\n",
      "Number of depth samples:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:15<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Plane Sweep Calculation\n",
      "Dense Map calculation\n",
      "Using trunc linear\n",
      "Finished solving\n"
     ]
    }
   ],
   "source": [
    "args = dict()\n",
    "args['min_depth'] = 1\n",
    "args['max_depth'] = 4\n",
    "args['patch_radius'] = 1\n",
    "args['iterations'] = 100\n",
    "args['std_p'] = (3,3)\n",
    "args['std_c'] = (20,20,20)\n",
    "args['wt'] = .5\n",
    "args['penalty'] = .5\n",
    "args['folder'] = \"custom_still\"\n",
    "args['num'] = 5\n",
    "args['wta'] = True\n",
    "# dense_depth(args)\n",
    "model = DenseReconstruction(args)\n",
    "model.create_reference()\n",
    "model.depth_utility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseReconstruction :\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.folder = args['folder'].split(\"_\")[0]\n",
    "        self.num_samples = int(args['num'])\n",
    "        self.wta = args['wta']\n",
    "        self.min_depth = float(args['min_depth'])\n",
    "        self.patch_radius = int(args['patch_radius'])\n",
    "        self.scale = 1\n",
    "        self.iterations = args['iterations']\n",
    "        self.max_depth = float(args['max_depth'])\n",
    "        self.depth_samples = np.zeros(int(args['num']))\n",
    "        self.ref = cv2.imread(IMAGES_FOLDER_PATH+\"/\"+self.args['folder']+\"_1.jpg\")\n",
    "        self.outfile = '../output/cost_volume_'+str(self.depth_samples.shape[0])+'__'+str(args[\"std_c\"])+'__depth_map.png'\n",
    "        self.step = (1.0) / (self.num_samples - 1.0)\n",
    "        self.pc = 0\n",
    "        self.wt = args['wt']\n",
    "        self.std_c = args['std_c']\n",
    "        self.std_p = args['std_p']\n",
    "        self.max_penalty = args['penalty']\n",
    "        self.ratio = .55\n",
    "    def create_reference(self):\n",
    "        self.ref = cv2.pyrDown(self.ref)\n",
    "        # Mean shifting image\n",
    "        self.ref = cv2.pyrMeanShiftFiltering(self.ref, 20, 20, 1)\n",
    "        self.ref = cv2.cvtColor(self.ref, cv2.COLOR_BGR2Lab)\n",
    "    def depth_utility(self):\n",
    "        for val,_ in zip(range(int(args['num'])),range(int(args['num']))):\n",
    "            sample = (self.max_depth * self.min_depth) / (self.max_depth - (self.max_depth - self.min_depth) * val * self.step)\n",
    "            self.depth_samples[val] = CAMERA_PARAMS['fx']/sample\n",
    "        print(\"Doing Plane Sweep Calculation for photoconsistency\")\n",
    "#         self.pc = self.plane_sweep_util()\n",
    "        self.plane_sweep_util()\n",
    "        print(\"Finished Plane Sweep Calculation\")\n",
    "        print(\"Dense Map calculation\")\n",
    "        self.Dense_Construction()\n",
    "        print(\"Finished solving\")\n",
    "    def plane_sweep_util(self):\n",
    "        print(\"Number of depth samples: \",self.depth_samples.shape[0])\n",
    "        C = []\n",
    "        R = []\n",
    "        if(self.wta):\n",
    "            K = np.array([\n",
    "            [CAMERA_PARAMS['fx'], CAMERA_PARAMS['s'], CAMERA_PARAMS['cx']],\n",
    "            [ 0.0,CAMERA_PARAMS['fy'],CAMERA_PARAMS['cy']],\n",
    "            [ 0.0, 0.0,1.0],])\n",
    "        \n",
    "        # Get extrinsics\n",
    "        with open(EXTRINSIC_FILE) as ext_file:\n",
    "            csv_reader = csv.reader(ext_file, delimiter=',')\n",
    "            if(self.wta):\n",
    "                for row in csv_reader:\n",
    "                    p = [float(r) for r in row[:-1]]\n",
    "                    rot, _ = cv2.Rodrigues(np.array(p[:3]))\n",
    "                    trans = np.array(p[3:6])\n",
    "                    c = -1 * np.linalg.inv(rot) @ trans\n",
    "                    R.append(rot)\n",
    "                    C.append(c)\n",
    "\n",
    "        # Get all images\n",
    "        all_img = []\n",
    "        scaled_gray_images = []\n",
    "#         print(\"test2\",len(R))\n",
    "        for file in sorted(os.listdir(IMAGES_FOLDER_PATH))[:len(R)] :  # Get as many images as the extrinsics available\n",
    "            if file.endswith('.jpg') :\n",
    "                im = cv2.imread(os.path.join(IMAGES_FOLDER_PATH, file))\n",
    "                im = im.astype(np.float32)\n",
    "                all_img.append(im)\n",
    "#         print(\"test3\",len(all_img))\n",
    "        for img in all_img :\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            gray_img = cv2.pyrDown(gray_img)\n",
    "            scaled_gray_images.append(gray_img)\n",
    "        if(self.wta):\n",
    "            ref_img = scaled_gray_images[0]\n",
    "            height, width = ref_img.shape\n",
    "\n",
    "        num_images = len(all_img)\n",
    "        cost_volume_arr = np.zeros((self.depth_samples.shape[0], height, width))\n",
    "\n",
    "        for idx, depth in enumerate(tqdm(self.depth_samples)):\n",
    "\n",
    "            homographies = np.zeros((num_images, 3, 3))\n",
    "            warped_images = []\n",
    "\n",
    "            for ind,_ in zip(range(num_images),range(num_images)) :\n",
    "                scale = 1\n",
    "                h = self.HomographyFrom(K, C[0], R[0], C[ind], R[ind], depth)\n",
    "                actual_scale = 2\n",
    "                if(self.wta):\n",
    "                    h[:,:2] = h[:,:2]*actual_scale\n",
    "                    h[2,:] *= actual_scale\n",
    "                    homographies[ind,:,:] = h\n",
    "\n",
    "            for i,_ in zip(range(1, num_images),range(num_images-1)):\n",
    "                warp = cv2.warpPerspective(scaled_gray_images[i], homographies[i], ref_img.shape[::-1], cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "                warped_images.append(warp)\n",
    "\n",
    "            if(self.wta):\n",
    "                ref_img_patches = as_strided(ref_img, shape=(ref_img.shape[0] - 2*self.patch_radius,ref_img.shape[1] - 2*self.patch_radius, 2*self.patch_radius + 1, 2*self.patch_radius + 1),strides=ref_img.strides + ref_img.strides, writeable=False)\n",
    "\n",
    "            h, w, _, _ = ref_img_patches.shape\n",
    "            patch_size = 2*self.patch_radius + 1\n",
    "            patch_radius = copy.deepcopy(self.patch_radius)\n",
    "            if(patch_radius):\n",
    "                ref_img_patches = ref_img_patches.reshape((ref_img_patches.shape[0]*ref_img_patches.shape[1], patch_size**2))\n",
    "                warp_patches = np.zeros((len(warped_images), ref_img_patches.shape[0], ref_img_patches.shape[1]))\n",
    "            for i,_ in zip(range(len(warped_images)),range(len(warped_images))):\n",
    "                if(self.wta):\n",
    "                    x = as_strided(warped_images[i], shape=(warped_images[i].shape[0] - 2*self.patch_radius,\n",
    "                                    warped_images[i].shape[1] - 2*self.patch_radius, 2*self.patch_radius + 1, 2*self.patch_radius + 1),\n",
    "                                    strides=warped_images[i].strides + warped_images[i].strides, writeable=False)\n",
    "                    self.patch_size = copy.deepcopy(patch_size)\n",
    "                    x = x.reshape((x.shape[0]*x.shape[1], patch_size**2))\n",
    "                    warp_patches[i,:,:] = x\n",
    "\n",
    "            L1_diff = self.Sad(ref_img_patches, warp_patches)\n",
    "            score = self.MergeScores(L1_diff, valid_ratio = 0.5)\n",
    "            if(self.wta):\n",
    "                patch_radius = copy.deepcopy(self.patch_radius)\n",
    "            # Border pixels take values of the neighboring pixels\n",
    "                cost_volume_arr[idx, self.patch_radius:height-self.patch_radius, self.patch_radius:width-self.patch_radius] = score.reshape((h,w))\n",
    "                patch_radius = copy.deepcopy(self.patch_radius)\n",
    "                cost_volume_arr[idx, 0: self.patch_radius, :] = cost_volume_arr[idx, self.patch_radius, :]\n",
    "                if(self.wta):\n",
    "                cost_volume_arr[idx, height-patch_radius+1:, :] = cost_volume_arr[idx, height-patch_radius, :]\n",
    "            \n",
    "            patch_radius = copy.deepcopy(self.patch_radius)\n",
    "            cost_volume_arr[idx, :, 0: patch_radius] = cost_volume_arr[idx, :, patch_radius].reshape((cost_volume_arr[idx, :, patch_radius].shape[0],1))\n",
    "            cost_volume_arr[idx, :, width-patch_radius+1:] = cost_volume_arr[idx, :, width-patch_radius].reshape((cost_volume_arr[idx, :, width-patch_radius].shape[0],1))\n",
    "\n",
    "        cost_volume_arr = self.Modulate(cost_volume_arr)\n",
    "\n",
    "        # Saving convention\n",
    "        #     np.savez_compressed(outfile, pc_cost=cost_volume_arr, dir=folder, max_d=max_depth, min_d=min_depth)\n",
    "\n",
    "        self.pc =  cost_volume_arr.astype('float32')\n",
    "\n",
    "    def Sad(self,ref_patch, warp_patch) :\n",
    "        return np.sum(np.abs(warp_patch - ref_patch), axis=2)\n",
    "\n",
    "    def HomographyFrom(self,K, C1, R1, C2, R2, dep):\n",
    "\n",
    "        # C1, R1 : Reference Image\n",
    "        H  = dep * K @ R2 @ R1.T @ np.linalg.inv(K)\n",
    "        H[:,2] += K @ R2 @ (C1 - C2)\n",
    "        return H\n",
    "\n",
    "\n",
    "    def MergeScores(self,scores):\n",
    "        '''\n",
    "        Takes the average of top k values in array. k == valid_scores.\n",
    "        '''\n",
    "        self.num_valid_scores = int(scores.shape[0] * self.ratio)\n",
    "        if(self.wta):\n",
    "            ix = np.argpartition(scores, self.num_valid_scores, axis=0)\n",
    "            ix = ix[:self.num_valid_scores,:]\n",
    "        if(self.wta):\n",
    "            srt = np.take_along_axis(scores, ix, axis=0)\n",
    "\n",
    "        return (np.sum(srt, axis=0) / self.num_valid_scores)\n",
    "\n",
    "    def GetMin(self,values):\n",
    "\n",
    "        f = s = 0\n",
    "        f, s = np.partition(values, 1)[0:2]\n",
    "        return f, s\n",
    "\n",
    "\n",
    "    def Modulate(self,cost_volume_arr):\n",
    "\n",
    "        first = 0\n",
    "        second = 0\n",
    "        confidence = 0\n",
    "        num_samples = cost_volume_arr.shape[0]\n",
    "\n",
    "        for r,_ in zip(range(cost_volume_arr.shape[1]),range(cost_volume_arr.shape[1])):\n",
    "            for _,c in zip(range(cost_volume_arr.shape[2]),range(cost_volume_arr.shape[2])):\n",
    "                if(self.wta):\n",
    "                    values = cost_volume_arr[:, r, c]\n",
    "                    first, second = self.GetMin(values, num_samples)\n",
    "                    confidence = (second + 1) / (first + 1)\n",
    "                    cost_volume_arr[:, r, c] = values * confidence\n",
    "        return cost_volume_arr\n",
    "    def Dense_Construction(self):\n",
    "        labels = self.pc.shape[0]\n",
    "#         iters = params['iterations']\n",
    "#         weight = params['wt']\n",
    "#         pos_std = params['std_p']\n",
    "#         rgb_std = params['std_p']\n",
    "#         max_penalty = params['max_penalty']\n",
    "#         print(\"test4\")\n",
    "        # Get initial crude depth map from photoconsistency\n",
    "        if self.wta :\n",
    "            self.compute_unary_image(self.pc)\n",
    "#         print(\"test5\")\n",
    "        # Normalize values for each pixel location\n",
    "        for r,_ in zip(range(self.pc.shape[1]),range(self.pc.shape[1])):\n",
    "            for _,c in zip(range(self.pc.shape[2]),range(self.pc.shape[2])):\n",
    "                if np.sum(self.pc[:, r, c]) <= 1e-9:\n",
    "                    self.pc[:, r, c] = 0.0\n",
    "                else:\n",
    "                    self.pc[:, r, c] = self.pc[:, r, c]/np.sum(self.pc[:, r, c])\n",
    "#         print(\"test6\")\n",
    "        # Convert to class probabilities for each pixel location\n",
    "        unary = unary_from_softmax(self.pc)\n",
    "\n",
    "        d = dcrf.DenseCRF2D(self.ref.shape[1], self.ref.shape[0], labels)\n",
    "\n",
    "        # Add photoconsistency score as uanry potential. 16-size vector\n",
    "        # for each pixel location\n",
    "        d.setUnaryEnergy(unary)\n",
    "        # Add color-dependent term, i.e. features are (x,y,r,g,b)\n",
    "        d.addPairwiseBilateral(sxy=self.std_p, srgb=self.std_c, rgbim=self.ref, compat=np.array([self.wt, labels*self.max_penalty]), kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "        # Run inference steps\n",
    "        Q = d.inference(self.iterations)\n",
    "\n",
    "        # Extract depth values. Map to [0-255]\n",
    "        self.MAP = np.argmax(Q, axis=0).reshape((self.ref.shape[:2]))\n",
    "        if(self.wta):\n",
    "            self.depth_map = np.zeros((self.MAP.shape[0], self.MAP.shape[1]))\n",
    "\n",
    "        for i,_ in zip(range(self.MAP.shape[0]),range(self.MAP.shape[0])):\n",
    "            for j,_ in zip(range(self.MAP.shape[1]),range(self.MAP.shape[0])):\n",
    "                self.depth_map[i,j] = self.depth_samples[self.MAP[i,j]]\n",
    "\n",
    "        min_val = np.min(self.depth_map)\n",
    "        max_val = np.max(self.depth_map)\n",
    "\n",
    "        for i in range(self.MAP.shape[0]):\n",
    "            for j in range(self.MAP.shape[1]):\n",
    "                self.depth_map[i,j] = ((self.depth_map[i,j] - min_val)/(max_val - min_val)) * (255)\n",
    "\n",
    "        # Upsampling depth map\n",
    "        cv2.imwrite(self.outfile, self.depth_map)\n",
    "    def compute_unary_image(self,unary):\n",
    "#         print(\"check\",unary.shape,depth_samples.shape)\n",
    "        gd = np.argmin(unary, axis=0)\n",
    "        gd_im = np.zeros((unary.shape[1], unary.shape[2]))\n",
    "        for i,_ in zip(range(unary.shape[1]),range(unary.shape[1])):\n",
    "            for j,_ in zip(range(unary.shape[2]),range(unary.shape[1])):\n",
    "                gd_im[i,j] = ((self.depth_samples[gd[i,j]] - np.min(self.depth_samples)) * 255.0) / (np.max(self.depth_samples) - np.min(self.depth_samples))\n",
    "#         cv2.imwrite(outfile, gd_im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
