{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import unary_from_labels, create_pairwise_bilateral, create_pairwise_gaussian,unary_from_softmax\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_FOLDER_PATH = \"../datasets/custom_still\"\n",
    "\n",
    "\n",
    "# Point Clouds \n",
    "INITIAL_POINT_CLOUD = '../output/initial_point_cloud.ply'\n",
    "FINAL_POINT_CLOUD = '../output/final_point_cloud.ply'\n",
    "\n",
    "# Bundle File\n",
    "BUNDLE_FILE = '../output/bundle.out'\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "feature_params = dict(maxCorners = 5000, \n",
    "                      qualityLevel = 0.03, \n",
    "                      minDistance = 10, \n",
    "                      blockSize = 15\n",
    "                      )\n",
    "\n",
    "# Lucas-Kanade parameters\n",
    "lk_params = dict(   winSize  = (25,25),\n",
    "                    maxLevel = 8,\n",
    "                    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.3))\n",
    "# Ceres-Solver parameters\n",
    "CERES_PARAMS = dict(\n",
    "                    solver = '../ceres-bin/bin/bundle_adjuster',\n",
    "                    maxIterations = 1000,\n",
    "                    input_ply = '../output/initial.ply',\n",
    "                    output_ply = '../output/final.ply',\n",
    "                    inner_iterations = 'true',\n",
    "                    nonmonotonic_steps = 'false'\n",
    "                    )\n",
    "\n",
    "CAMERA_PARAMS = dict(fx=1781,\n",
    "                     fy=1781,\n",
    "                     cx=960,\n",
    "                     cy=540,\n",
    "                     k1=0,\n",
    "                     k2=0,\n",
    "                     s=0,\n",
    "                    )\n",
    "EXTRINSIC_FILE = '../output/extrinsics.csv'\n",
    "\n",
    "# Initial Point Cloud\n",
    "INITIAL_POINT_CLOUD = '../output/initial_point_cloud.ply'\n",
    "\n",
    "# FINAL Point Cloud\n",
    "FINAL_POINT_CLOUD = '../output/final_point_cloud.ply'\n",
    "\n",
    "# Bundle File\n",
    "BUNDLE_FILE = '../output/bundle.out'\n",
    "\n",
    "# Optical Flow Plot\n",
    "OPTICAL_FLOW_PLOT = '../output/optical_flow.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_camera_matrix(camera_params):\n",
    "    K = np.array([\n",
    "        [camera_params['fx'],  camera_params['s'], camera_params['cx']],\n",
    "        [                  0, camera_params['fy'], camera_params['cy']],\n",
    "        [                  0,                   0,                  1],\n",
    "    ])\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sad(ref_patch, warp_patch) :\n",
    "\n",
    "    '''\n",
    "    Calculates L1 Loss between two grayscale image patches\n",
    "    N : Total number of warp images\n",
    "    P : Total number of patches per image\n",
    "    w : Length of one side of patch\n",
    "\n",
    "    Dimension of patch ndarray : N x P x (w*w)\n",
    "    Returned array dim : N x P\n",
    "    '''\n",
    "\n",
    "    err = np.sum(np.abs(warp_patch - ref_patch), axis=2)\n",
    "    return err\n",
    "\n",
    "\n",
    "def HomographyFrom(K, C1, R1, C2, R2, dep):\n",
    "\n",
    "    # C1, R1 : Reference Image\n",
    "    H  = dep * K @ R2 @ R1.T @ np.linalg.inv(K)\n",
    "    H[:,2] += K @ R2 @ (C1 - C2)\n",
    "    return H\n",
    "\n",
    "\n",
    "def MergeScores(scores, valid_ratio = 0.5):\n",
    "    '''\n",
    "    Takes the average of top k values in array. k == valid_scores.\n",
    "    N : Total number of warp images\n",
    "    P : Total number of patches per image\n",
    "\n",
    "    Dimension of scores array: N x P\n",
    "    Dimension of returned array: (N*valid_ratio) x P\n",
    "    '''\n",
    "\n",
    "    num_valid_scores = int(scores.shape[0] * valid_ratio)\n",
    "\n",
    "    ix = np.argpartition(scores, num_valid_scores, axis=0)\n",
    "    ix = ix[:num_valid_scores,:]\n",
    "\n",
    "    srt = np.take_along_axis(scores, ix, axis=0)\n",
    "    score = np.sum(srt, axis=0) / num_valid_scores\n",
    "\n",
    "    return score\n",
    "\n",
    "def GetMin(values, size):\n",
    "    '''\n",
    "    Get smallest two values in array\n",
    "    '''\n",
    "\n",
    "    assert(size>1)\n",
    "\n",
    "    f = 0\n",
    "    s = 0\n",
    "\n",
    "    f, s = np.partition(values, 1)[0:2]\n",
    "\n",
    "    return f, s\n",
    "\n",
    "\n",
    "def Modulate(cost_volume_arr):\n",
    "\n",
    "    first = 0\n",
    "    second = 0\n",
    "    confidence = 0\n",
    "    num_samples = cost_volume_arr.shape[0]\n",
    "\n",
    "    for r in range(cost_volume_arr.shape[1]):\n",
    "        for c in range(cost_volume_arr.shape[2]):\n",
    "\n",
    "            values = cost_volume_arr[:, r, c]\n",
    "            first, second = GetMin(values, num_samples)\n",
    "            confidence = (second + 1) / (first + 1)\n",
    "            cost_volume_arr[:, r, c] = values * confidence\n",
    "\n",
    "    return cost_volume_arr\n",
    "\n",
    "def plane_sweep(folder, depth_samples, min_depth, max_depth, scale, patch_radius):\n",
    "\n",
    "    print(f\"Number of depth samples: {depth_samples.shape[0]}\")\n",
    "\n",
    "    # Intrinsics, Camera centers, Rotation mtx\n",
    "    K = construct_camera_matrix(CAMERA_PARAMS)\n",
    "    C = []\n",
    "    R = []\n",
    "\n",
    "    # Get extrinsics\n",
    "    with open(EXTRINSIC_FILE) as ext_file:\n",
    "        csv_reader = csv.reader(ext_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "\n",
    "            p = [float(r) for r in row[:-1]]\n",
    "            rot, _ = cv2.Rodrigues(np.array(p[:3]))\n",
    "            trans = np.array(p[3:6])\n",
    "            c = -1 * np.linalg.inv(rot) @ trans\n",
    "\n",
    "            C.append(c)\n",
    "            R.append(rot)\n",
    "\n",
    "    # Get all images\n",
    "    all_img = []\n",
    "    print(\"test2\",len(R))\n",
    "    for file in sorted(os.listdir(IMAGES_FOLDER_PATH))[:len(R)] :  # Get as many images as the extrinsics available\n",
    "\n",
    "        if file.endswith('.png') or file.endswith('.jpg') :\n",
    "            im = cv2.imread(os.path.join(IMAGES_FOLDER_PATH, file))\n",
    "            all_img.append(im)\n",
    "    print(\"test3\",len(all_img))\n",
    "    scaled_gray_images = []\n",
    "    for img in all_img :\n",
    "        img = img.astype(np.float32)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        for s in range(scale):\n",
    "            gray_img = cv2.pyrDown(gray_img)\n",
    "\n",
    "        scaled_gray_images.append(gray_img)\n",
    "\n",
    "    ref_img = scaled_gray_images[0]\n",
    "    height, width = ref_img.shape\n",
    "\n",
    "    num_images = len(all_img)\n",
    "    cost_volume_arr = np.zeros((depth_samples.shape[0], height, width))\n",
    "\n",
    "    for idx, depth in enumerate(tqdm(depth_samples)):\n",
    "\n",
    "        homographies = np.zeros((num_images, 3, 3))\n",
    "        warped_images = []\n",
    "\n",
    "        for ind in range(num_images) :\n",
    "\n",
    "            h = HomographyFrom(K, C[0], R[0], C[ind], R[ind], depth)\n",
    "            actual_scale = 2**scale\n",
    "            h[:,:2] *= actual_scale\n",
    "            h[2,:] *= actual_scale\n",
    "            homographies[ind,:,:] = h\n",
    "\n",
    "        # Assume 0th image is reference image\n",
    "        for i in range(1, num_images):\n",
    "            warp = cv2.warpPerspective(scaled_gray_images[i], homographies[i], ref_img.shape[::-1], cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "            warped_images.append(warp)\n",
    "\n",
    "\n",
    "        ref_img_patches = as_strided(ref_img, shape=(ref_img.shape[0] - 2*patch_radius,\n",
    "                                    ref_img.shape[1] - 2*patch_radius, 2*patch_radius + 1, 2*patch_radius + 1),\n",
    "                                    strides=ref_img.strides + ref_img.strides, writeable=False)\n",
    "\n",
    "        h, w, _, _ = ref_img_patches.shape\n",
    "        patch_size = 2*patch_radius + 1\n",
    "        ref_img_patches = ref_img_patches.reshape((ref_img_patches.shape[0]*ref_img_patches.shape[1], patch_size**2))\n",
    "        warp_patches = np.zeros((len(warped_images), ref_img_patches.shape[0], ref_img_patches.shape[1]))\n",
    "        for i in range(len(warped_images)):\n",
    "\n",
    "            x = as_strided(warped_images[i], shape=(warped_images[i].shape[0] - 2*patch_radius,\n",
    "                            warped_images[i].shape[1] - 2*patch_radius, 2*patch_radius + 1, 2*patch_radius + 1),\n",
    "                            strides=warped_images[i].strides + warped_images[i].strides, writeable=False)\n",
    "\n",
    "            x = x.reshape((x.shape[0]*x.shape[1], patch_size**2))\n",
    "            warp_patches[i,:,:] = x\n",
    "\n",
    "        L1_diff = Sad(ref_img_patches, warp_patches)\n",
    "        score = MergeScores(L1_diff, valid_ratio = 0.5)\n",
    "\n",
    "        # Border pixels take values of the neighboring pixels\n",
    "        cost_volume_arr[idx, patch_radius:height-patch_radius, patch_radius:width-patch_radius] = score.reshape((h,w))\n",
    "        cost_volume_arr[idx, 0: patch_radius, :] = cost_volume_arr[idx, patch_radius, :]\n",
    "        cost_volume_arr[idx, height-patch_radius+1:, :] = cost_volume_arr[idx, height-patch_radius, :]\n",
    "        cost_volume_arr[idx, :, 0: patch_radius] = cost_volume_arr[idx, :, patch_radius].reshape((cost_volume_arr[idx, :, patch_radius].shape[0],1))\n",
    "        cost_volume_arr[idx, :, width-patch_radius+1:] = cost_volume_arr[idx, :, width-patch_radius].reshape((cost_volume_arr[idx, :, width-patch_radius].shape[0],1))\n",
    "\n",
    "    cost_volume_arr = Modulate(cost_volume_arr)\n",
    "\n",
    "    # Saving convention\n",
    "#     np.savez_compressed(outfile, pc_cost=cost_volume_arr, dir=folder, max_d=max_depth, min_d=min_depth)\n",
    "\n",
    "    return cost_volume_arr.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unary_image(unary, depth_samples, outfile):\n",
    "    print(\"check\",unary.shape,depth_samples.shape)\n",
    "    gd = np.argmin(unary, axis=0)\n",
    "    gd_im = np.zeros((unary.shape[1], unary.shape[2]))\n",
    "    for i in range(unary.shape[1]):\n",
    "        for j in range(unary.shape[2]):\n",
    "            gd_im[i,j] = ((depth_samples[gd[i,j]] - np.min(depth_samples)) * 255.0) / (np.max(depth_samples) - np.min(depth_samples))\n",
    "\n",
    "    cv2.imwrite(outfile, gd_im)\n",
    "\n",
    "def DenseCRF(unary, img, depth_samples, params, folder, max_depth, min_depth, outfile='depth_map.png', show_wta=False):\n",
    "\n",
    "    labels = unary.shape[0]\n",
    "    iters = params['iterations']\n",
    "    weight = params['wt']\n",
    "    pos_std = params['std_p']\n",
    "    rgb_std = params['std_p']\n",
    "    max_penalty = params['max_penalty']\n",
    "    print(\"test4\")\n",
    "    # Get initial crude depth map from photoconsistency\n",
    "    if show_wta :\n",
    "        compute_unary_image(unary, depth_samples, outfile=f'../output/cost_volume_{depth_samples.shape[0]}_wta.png')\n",
    "    print(\"test5\")\n",
    "    # Normalize values for each pixel location\n",
    "    for r in range(unary.shape[1]):\n",
    "        for c in range(unary.shape[2]):\n",
    "            if np.sum(unary[:, r, c]) <= 1e-9:\n",
    "                unary[:, r, c] = 0.0\n",
    "            else:\n",
    "                unary[:, r, c] = unary[:, r, c]/np.sum(unary[:, r, c])\n",
    "    print(\"test6\")\n",
    "    # Convert to class probabilities for each pixel location\n",
    "    unary = unary_from_softmax(unary)\n",
    "\n",
    "    d = dcrf.DenseCRF2D(img.shape[1], img.shape[0], labels)\n",
    "\n",
    "    # Add photoconsistency score as uanry potential. 16-size vector\n",
    "    # for each pixel location\n",
    "    d.setUnaryEnergy(unary)\n",
    "    # Add color-dependent term, i.e. features are (x,y,r,g,b)\n",
    "    d.addPairwiseBilateral(sxy=pos_std, srgb=rgb_std, rgbim=img, compat=np.array([weight, labels*max_penalty]), kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    # Run inference steps\n",
    "    Q = d.inference(iters)\n",
    "\n",
    "    # Extract depth values. Map to [0-255]\n",
    "    MAP = np.argmax(Q, axis=0).reshape((img.shape[:2]))\n",
    "    depth_map = np.zeros((MAP.shape[0], MAP.shape[1]))\n",
    "\n",
    "    for i in range(MAP.shape[0]):\n",
    "        for j in range(MAP.shape[1]):\n",
    "            depth_map[i,j] = depth_samples[MAP[i,j]]\n",
    "\n",
    "    min_val = np.min(depth_map)\n",
    "    max_val = np.max(depth_map)\n",
    "\n",
    "    for i in range(MAP.shape[0]):\n",
    "        for j in range(MAP.shape[1]):\n",
    "            depth_map[i,j] = ((depth_map[i,j] - min_val)/(max_val - min_val)) * 255.0\n",
    "\n",
    "    # Upsampling depth map\n",
    "    cv2.imwrite(outfile, depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_depth(args) :\n",
    "\n",
    "    folder = args['folder'].split(\"_\")[0]\n",
    "    num_samples = int(args['num'])\n",
    "    wta = args['wta']\n",
    "\n",
    "    \n",
    "    min_depth = float(args['min_depth'])\n",
    "    patch_radius = int(args['patch_radius'])\n",
    "\n",
    "    pc_score = 0\n",
    "    \n",
    "    scale = int(args['scale'])\n",
    "    max_depth = float(args['max_depth'])\n",
    "    # Create depth samples in the specified depth range\n",
    "    depth_samples = np.zeros(int(args['num']))\n",
    "    step = 1.0 / (num_samples - 1.0)\n",
    "\n",
    "    for val in range(int(args['num'])):\n",
    "        sample = (max_depth * min_depth) / (max_depth - (max_depth - min_depth) * val * step)\n",
    "        depth_samples[val] = CAMERA_PARAMS['fx']/sample\n",
    "        # depth_samples[val] = sample\n",
    "\n",
    "    # Get reference image\n",
    "    file = args['folder'] + \"_1.jpg\"\n",
    "\n",
    "\n",
    "    ref_img = cv2.imread(IMAGES_FOLDER_PATH+\"/\"+file)\n",
    "#     print(ref_img.shape)\n",
    "    for s,_ in zip(range(scale),range(scale)):\n",
    "        ref_img = cv2.pyrDown(ref_img)\n",
    "    # Mean shifting image\n",
    "    ref_img = cv2.pyrMeanShiftFiltering(ref_img, 20, 20, 1)\n",
    "\n",
    "    ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "    # Perform plane sweep to calculate photo-consistency loss\n",
    "#     outfile = '../output/cost_volume_{depth_samples.shape[0]}'\n",
    "    print(\"Doing Plane Sweep Calculation for photoconsistency\")\n",
    "    pc_score = plane_sweep(folder, depth_samples, min_depth, max_depth, scale, patch_radius)\n",
    "    print(\"Finished Plane Sweep Calculation\")\n",
    "#     print(\"here\",pc_score)\n",
    "    outfile = '../output/cost_volume_{depth_samples.shape[0]}__{args[\"std_c\"]}_depth_map.png'\n",
    "#     crf_params = dict()\n",
    "#     crf_params['iters'] = int(args['iterations'])\n",
    "#     crf_params['pos_std'] = tuple(float(x) for x in args['std_p'].split(','))\n",
    "#     crf_params['rgb_std'] = tuple(float(x) for x in args['std_c'].split(','))\n",
    "#     crf_params['weight'] = float(args['wt'])\n",
    "#     crf_params['max_penalty'] = float(args['penalty'])\n",
    "\n",
    "    # Use photoconsistency score as unary potential\n",
    "    print(\"Dense Map calculation\")\n",
    "    depth_map = DenseCRF(pc_score, ref_img, depth_samples,args, folder, max_depth, min_depth, outfile, wta)\n",
    "    print(\"Finished solving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Plane Sweep Calculation for photoconsistency\n",
      "Number of depth samples: 5\n",
      "test2 136\n",
      "test3 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:14<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Plane Sweep Calculation\n",
      "Dense Map calculation\n",
      "test4\n",
      "check (5, 480, 480) (5,)\n",
      "test5\n",
      "test6\n",
      "Using trunc linear\n",
      "Finished solving\n"
     ]
    }
   ],
   "source": [
    "args = dict()\n",
    "args['min_depth'] = 1\n",
    "args['max_depth'] = 4\n",
    "args['patch_radius'] = 1\n",
    "args['iterations'] = 100\n",
    "args['std_p'] = (3,3)\n",
    "args['std_c'] = (20,20,20)\n",
    "args['wt'] = .5\n",
    "args['penalty'] = .5\n",
    "args['folder'] = \"custom_still\"\n",
    "args['num'] = 5\n",
    "args['wta'] = True\n",
    "dense_depth(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseReconstruction :\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.folder = args['folder'].split(\"_\")[0]\n",
    "        self.num_samples = int(args['num'])\n",
    "        self.wta = args['wta']\n",
    "        self.min_depth = float(args['min_depth'])\n",
    "        self.patch_radius = int(args['patch_radius'])\n",
    "        self.scale = 1\n",
    "        self.max_depth = float(args['max_depth'])\n",
    "        self.depth_samples = np.zeros(int(args['num']))\n",
    "        self.ref = cv2.imread(IMAGES_FOLDER_PATH+\"/\"+self.args['folder']+\"_1.jpg\"\n",
    "        self.outfile = '../output/cost_volume_{depth_samples.shape[0]}__{args[\"std_c\"]}_depth_map.png'\n",
    "        self.step = (1.0) / (self.num_samples - 1.0)\n",
    "        self.pc = 0\n",
    "    def create_reference(self):\n",
    "        self.ref = cv2.pyrDown(self.ref)\n",
    "        # Mean shifting image\n",
    "        self.ref = cv2.pyrMeanShiftFiltering(self.ref, 20, 20, 1)\n",
    "        self.ref = cv2.cvtColor(self.red, cv2.COLOR_BGR2Lab)\n",
    "    def depth_utility(self):\n",
    "        for val,_ in zip(range(int(args['num'])),range(int(args['num']))):\n",
    "            sample = (self.max_depth * self.min_depth) / (self.max_depth - (self.max_depth - min_depth) * val * self.step)\n",
    "            self.depth_samples[val] = CAMERA_PARAMS['fx']/sample\n",
    "        print(\"Doing Plane Sweep Calculation for photoconsistency\")\n",
    "#         self.pc = self.plane_sweep_util()\n",
    "        self.plane_sweep_util()\n",
    "        print(\"Finished Plane Sweep Calculation\")\n",
    "        print(\"Dense Map calculation\")\n",
    "        self.Dense_Construction()\n",
    "        print(\"Finished solving\")\n",
    "    def plane_sweep_util(self):\n",
    "        print(f\"Number of depth samples: {depth_samples.shape[0]}\")\n",
    "        C = []\n",
    "        R = []\n",
    "        if(sef.wta):\n",
    "            K = np.array([\n",
    "            [CAMERA_PARAMS['fx'], CAMERA_PARAMS['s'], CAMERA_PARAMS['cx']],\n",
    "            [ 0.0,CAMERA_PARAMS['fy'],CAMERA_PARAMS['cy']],\n",
    "            [ 0.0, 0.0,1.0],])\n",
    "        \n",
    "        # Get extrinsics\n",
    "        with open(EXTRINSIC_FILE) as ext_file:\n",
    "            csv_reader = csv.reader(ext_file, delimiter=',')\n",
    "            if(self.wta):\n",
    "                for row in csv_reader:\n",
    "                    p = [float(r) for r in row[:-1]]\n",
    "                    rot, _ = cv2.Rodrigues(np.array(p[:3]))\n",
    "                    trans = np.array(p[3:6])\n",
    "                    c = -1 * np.linalg.inv(rot) @ trans\n",
    "                    R.append(rot)\n",
    "                    C.append(c)\n",
    "\n",
    "        # Get all images\n",
    "        all_img = []\n",
    "#         print(\"test2\",len(R))\n",
    "        for file in sorted(os.listdir(IMAGES_FOLDER_PATH))[:len(R)] :  # Get as many images as the extrinsics available\n",
    "            if file.endswith('.jpg') :\n",
    "                im = cv2.imread(os.path.join(IMAGES_FOLDER_PATH, file))\n",
    "                im = im.astype(np.float32)\n",
    "                all_img.append(im)\n",
    "#         print(\"test3\",len(all_img))\n",
    "        scaled_gray_images = []\n",
    "        for img in all_img :\n",
    "#             img = img.astype(np.float32)\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#             for _,_ in xip(range(self.scale),range(self.scale)):\n",
    "            gray_img = cv2.pyrDown(gray_img)\n",
    "\n",
    "            scaled_gray_images.append(gray_img)\n",
    "\n",
    "        ref_img = scaled_gray_images[0]\n",
    "        height, width = ref_img.shape\n",
    "\n",
    "        num_images = len(all_img)\n",
    "        cost_volume_arr = np.zeros((depth_samples.shape[0], height, width))\n",
    "\n",
    "        for idx, depth in enumerate(tqdm(self.depth_samples)):\n",
    "\n",
    "            homographies = np.zeros((num_images, 3, 3))\n",
    "            warped_images = []\n",
    "\n",
    "            for ind,_ in zip(range(num_images),range(num_images)) :\n",
    "                scale = 1\n",
    "                h = HomographyFrom(K, C[0], R[0], C[ind], R[ind], depth)\n",
    "                actual_scale = 2**scale\n",
    "                if(self.wta):\n",
    "                    h[:,:2] *= actual_scale\n",
    "                    h[2,:] *= actual_scale\n",
    "                    homographies[ind,:,:] = h\n",
    "\n",
    "            # Assume 0th image is reference image\n",
    "            for i,_ in zip(range(1, num_images),range(num_images-1)):\n",
    "                warp = cv2.warpPerspective(scaled_gray_images[i], homographies[i], ref_img.shape[::-1], cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "                warped_images.append(warp)\n",
    "\n",
    "            if(self.wta):\n",
    "                ref_img_patches = as_strided(ref_img, shape=(ref_img.shape[0] - 2*self.patch_radius,\n",
    "                                        ref_img.shape[1] - 2*self.patch_radius, 2*self.patch_radius + 1, 2*self.patch_radius + 1),\n",
    "                                        strides=ref_img.strides + ref_img.strides, writeable=False)\n",
    "\n",
    "            h, w, _, _ = ref_img_patches.shape\n",
    "            patch_size = 2*self.patch_radius + 1\n",
    "            ref_img_patches = ref_img_patches.reshape((ref_img_patches.shape[0]*ref_img_patches.shape[1], patch_size**2))\n",
    "            warp_patches = np.zeros((len(warped_images), ref_img_patches.shape[0], ref_img_patches.shape[1]))\n",
    "            for i in range(len(warped_images)):\n",
    "                if(self.wta):\n",
    "                    x = as_strided(warped_images[i], shape=(warped_images[i].shape[0] - 2*self.patch_radius,\n",
    "                                    warped_images[i].shape[1] - 2*self.patch_radius, 2*self.patch_radius + 1, 2*self.patch_radius + 1),\n",
    "                                    strides=warped_images[i].strides + warped_images[i].strides, writeable=False)\n",
    "\n",
    "                    x = x.reshape((x.shape[0]*x.shape[1], patch_size**2))\n",
    "                    warp_patches[i,:,:] = x\n",
    "\n",
    "            L1_diff = self.Sad(ref_img_patches, warp_patches)\n",
    "            score = self.MergeScores(L1_diff, valid_ratio = 0.5)\n",
    "            if(self.wta):\n",
    "                patch_radius = copy.deepcopy(self.patch_radius)\n",
    "            # Border pixels take values of the neighboring pixels\n",
    "                cost_volume_arr[idx, self.patch_radius:height-self.patch_radius, self.patch_radius:width-self.patch_radius] = score.reshape((h,w))\n",
    "                patch_radius = copy.deepcopy(self.patch_radius)\n",
    "                cost_volume_arr[idx, 0: self.patch_radius, :] = cost_volume_arr[idx, self.patch_radius, :]\n",
    "                cost_volume_arr[idx, height-patch_radius+1:, :] = cost_volume_arr[idx, height-patch_radius, :]\n",
    "                patch_radius = copy.deepcopy(self.patch_radius)\n",
    "                cost_volume_arr[idx, :, 0: patch_radius] = cost_volume_arr[idx, :, patch_radius].reshape((cost_volume_arr[idx, :, patch_radius].shape[0],1))\n",
    "                cost_volume_arr[idx, :, width-patch_radius+1:] = cost_volume_arr[idx, :, width-patch_radius].reshape((cost_volume_arr[idx, :, width-patch_radius].shape[0],1))\n",
    "\n",
    "        cost_volume_arr = self.Modulate(cost_volume_arr)\n",
    "\n",
    "        # Saving convention\n",
    "        #     np.savez_compressed(outfile, pc_cost=cost_volume_arr, dir=folder, max_d=max_depth, min_d=min_depth)\n",
    "\n",
    "        self.pc =  cost_volume_arr.astype('float32')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
